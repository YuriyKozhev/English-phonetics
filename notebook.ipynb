{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision.models as models\n",
    "import string\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', encoding='utf-8') as f:\n",
    "    train = f.readlines()\n",
    "train = np.array([np.array([k for k in c.split()]) for c in train])\n",
    "new_train = []\n",
    "new = []\n",
    "for i, t in enumerate(train):\n",
    "    if not i % 1000:\n",
    "        new = np.append(new, new_train)\n",
    "        new_train = []\n",
    "    word = t[0]\n",
    "    for i in range(1, t.size):\n",
    "        arr = np.append(word, t[i])\n",
    "        new_train = np.append(new_train, arr)\n",
    "new_train = new\n",
    "new_train = new_train.reshape(new_train.size//2, 2)\n",
    "train = new_train\n",
    "#print(train[:10], train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\" '-' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P'\n",
      " 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'Z&'] 29\n"
     ]
    }
   ],
   "source": [
    "all_letters = []\n",
    "for i, t in enumerate(train):\n",
    "    for c in t[0]:\n",
    "        all_letters = np.append(all_letters, c)\n",
    "    all_letters = np.unique(all_letters)\n",
    "end_letter = 'Z&'\n",
    "all_letters = np.append(all_letters, end_letter)\n",
    "all_letters = np.sort(all_letters)\n",
    "print(all_letters, all_letters.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEMIEUX\n",
      "tensor([13,  6, 14, 10,  6, 22, 25, 28]) torch.Size([8])\n",
      "LEMIEUX\n"
     ]
    }
   ],
   "source": [
    "n_letters = len(all_letters)\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.searchsorted(letter)\n",
    "\n",
    "def lineToIndices(line):\n",
    "    tensor = torch.zeros(len(line) + 1).type(torch.LongTensor)\n",
    "    for i, l in enumerate(line):\n",
    "        tensor[i] = int(letterToIndex(l))\n",
    "    tensor[-1] = int(letterToIndex(end_letter))\n",
    "    return tensor\n",
    "\n",
    "def indicesToLine(indices):\n",
    "    line = ''\n",
    "    for i in range(indices.size()[0] - 1): \n",
    "        line += all_letters[indices[i]]\n",
    "    return line\n",
    "\n",
    "sample_ = train[0][0]\n",
    "print(sample_)\n",
    "print(lineToIndices(sample_), lineToIndices(sample_).size())\n",
    "print(indicesToLine(lineToIndices(sample_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['###' 'AA' 'AE' 'AH' 'AO' 'AW' 'AY' 'B' 'CH' 'D' 'DH' 'EH' 'ER' 'EY' 'F'\n",
      " 'G' 'HH' 'IH' 'IY' 'JH' 'K' 'L' 'M' 'N' 'NG' 'OW' 'OY' 'P' 'R' 'S' 'SH'\n",
      " 'T' 'TH' 'UH' 'UW' 'V' 'W' 'Y' 'Z' 'ZH' 'ZZ$'] 41\n"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "for t in train:\n",
    "    categories = np.append(categories, t[1].split('_'))\n",
    "    categories = np.unique(categories)\n",
    "start_token = '###'\n",
    "end_token = 'ZZ$'\n",
    "categories = np.append(categories, (start_token, end_token))\n",
    "categories = np.sort(categories)\n",
    "print(categories, categories.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_AH_M_Y_UW\n",
      "tensor([ 0, 21,  3, 22, 37, 34, 40]) torch.Size([7])\n",
      "L_AH_M_Y_UW\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "n_categories = len(categories)\n",
    "\n",
    "def categoryToIndex(cat):\n",
    "    return categories.searchsorted(cat)\n",
    "\n",
    "def categoriesToIndices(line):\n",
    "    arr = line.split('_')\n",
    "    tensor = torch.zeros(len(arr) + 2)\n",
    "    tensor[0] = torch.tensor(categoryToIndex(start_token))\n",
    "    for li, cat in enumerate(arr):\n",
    "        tensor[li+1] = torch.tensor(categoryToIndex(cat))\n",
    "    tensor[-1] = torch.tensor(categoryToIndex(end_token))\n",
    "    return tensor.type(torch.LongTensor)\n",
    "\n",
    "def indicesToCategories(indeces):\n",
    "    line = ''\n",
    "    for i in range(1, indeces.size()[0] - 1): \n",
    "        line += categories[indeces[i]]\n",
    "        line += '_'\n",
    "    return line[:-1]\n",
    "\n",
    "def startToken():\n",
    "    return torch.tensor([categoryToIndex(start_token)]).type(torch.LongTensor)\n",
    "\n",
    "def endToken():\n",
    "    return torch.tensor([categoryToIndex(end_token)]).type(torch.LongTensor)\n",
    "\n",
    "    \n",
    "sample_ = train[0][1]\n",
    "print(sample_)\n",
    "print(categoriesToIndices(sample_), categoriesToIndices(sample_).size())\n",
    "print(indicesToCategories(categoriesToIndices(sample_)))\n",
    "print(startToken())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(29, 128)\n",
       "  (gru): GRU(128, 128)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "n_hidden = 128\n",
    "en = EncoderRNN(n_letters, n_hidden)\n",
    "en.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoderRNN(\n",
       "  (embedding): Embedding(41, 128)\n",
       "  (attn): Linear(in_features=256, out_features=34, bias=True)\n",
       "  (attn_combine): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (gru): GRU(128, 128)\n",
       "  (out): Linear(in_features=128, out_features=41, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "n_hidden = 128\n",
    "de = AttnDecoderRNN(n_hidden, n_categories)\n",
    "de.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "endTensor = endToken().to(device)\n",
    "\n",
    "def Train(input_tensor, target_tensor):\n",
    "    en_hidden = en.initHidden()\n",
    "\n",
    "    en_optimizer.zero_grad()\n",
    "    de_optimizer.zero_grad()\n",
    "    \n",
    "    input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    en_outputs = torch.zeros(MAX_LENGTH, en.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        en_output, en_hidden = en(input_tensor[ei], en_hidden)\n",
    "        en_outputs[ei] = en_output[0, 0]\n",
    "\n",
    "    de_input = startToken().to(device)\n",
    "\n",
    "    de_hidden = en_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "       \n",
    "        for di in range(target_length):\n",
    "            de_output, de_hidden, de_attention = de(de_input, de_hidden, en_outputs)\n",
    "            loss += criterion(de_output, target_tensor[di])\n",
    "            de_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            de_output, de_hidden, de_attention = de(de_input, de_hidden, en_outputs)\n",
    "            topv, topi = de_output.topk(1)\n",
    "            de_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(de_output, target_tensor[di])\n",
    "            if de_input.item() == endTensor.item():\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    en_optimizer.step()\n",
    "    de_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transcript(input_tensor):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        \n",
    "        input_length = input_tensor.size()[0]\n",
    "        en_hidden = en.initHidden()\n",
    "\n",
    "        en_outputs = torch.zeros(MAX_LENGTH, en.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            en_output, en_hidden = en(input_tensor[ei], en_hidden)\n",
    "            en_outputs[ei] += en_output[0, 0]\n",
    "\n",
    "        de_input = startToken().to(device)\n",
    "\n",
    "        de_hidden = en_hidden\n",
    "\n",
    "        decoded_categories = []\n",
    "\n",
    "        for di in range(MAX_LENGTH):\n",
    "            de_output, de_hidden, de_attention = de(de_input, de_hidden, en_outputs)\n",
    "            topv, topi = de_output.data.topk(1)\n",
    "            decoded_categories.append(topi.item())\n",
    "          #  print(topi.item())\n",
    "            if topi.item() == endTensor.item():\n",
    "                break\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(input_tensor, target_tensor):\n",
    "    output = np.array(Transcript(input_tensor))\n",
    "    target = target_tensor.data.numpy()\n",
    "  #  print(output, target)\n",
    "    return int(np.array_equal(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in  range(70000, 70000 + 100):\n",
    "    input_tensor, target_tensor = lineToIndices(train[i][0]), categoriesToIndices(train[i][1])\n",
    "    sum += Test(input_tensor, target_tensor)\n",
    "print(sum/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n",
    "epochs_num = 6\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation...\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "en_optimizer = torch.optim.SGD(en.parameters(), lr=0.1)\n",
    "de_optimizer = torch.optim.SGD(de.parameters(), lr=0.1)\n",
    "samples_num = 15000\n",
    "step = samples_num // 100\n",
    "for i in range(samples_num):\n",
    "    input_tensor, target_tensor = lineToIndices(train[i][0]), categoriesToIndices(train[i][1]).view(-1,1)\n",
    "    loss = Train(input_tensor, target_tensor)\n",
    "print('evaluation...')\n",
    "sum = 0\n",
    "num = 1000\n",
    "for i in  range(samples_num, samples_num + num):\n",
    "    input_tensor, target_tensor = lineToIndices(train[i][0]), categoriesToIndices(train[i][1])\n",
    "    sum += Test(input_tensor, target_tensor)\n",
    "print(sum/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36199\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40000 0.6920232772827148 [0, 27, 21, 5, 9, 40] tensor([ 0, 27, 21,  5,  9, 40])\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45000 0.6361077853611538 [0, 35, 3, 31, 17, 24, 24, 40] tensor([ 0, 35, 25, 31, 17, 24, 40])\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "evaluation...\n",
      "[0, 22, 12, 28, 40] tensor([ 0, 22, 12, 14, 40])\n",
      "0.09\n",
      "[0, 9, 17, 28, 17, 20, 31, 31, 31, 40] tensor([ 0,  9,  6, 28, 11, 20, 31, 17, 35, 38, 40])\n",
      "epoch: 2\n",
      "0\n",
      "0 1.664879662649972 [0, 21, 11, 22, 17, 34, 20, 20, 40] tensor([ 0, 21,  3, 22, 37, 34, 40])\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5000 0.3553908348083496 [0, 36, 18, 29, 40] tensor([ 0, 36, 18, 38, 40])\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10000 1.651793638865153 [0, 22, 17, 29, 3, 23, 23, 23, 40] tensor([ 0, 22, 17, 29, 31, 13, 31, 22,  3, 23, 31, 40])\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15000 0.503299331665039 [0, 11, 27, 29, 40] tensor([ 0, 11, 27, 29, 40])\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20000 1.5428341456821986 [0, 31, 28, 2, 23, 29, 27, 27, 27, 23, 40] tensor([ 0, 31, 28,  2, 23, 29, 27, 12, 31, 13, 30,  3, 23, 40])\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25000 0.6261165482657296 [0, 29, 17, 28, 25, 40] tensor([ 0, 29, 17, 28,  3, 23, 40])\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30000 2.197145462036133 [0, 37, 23, 15, 16, 15, 40] tensor([ 0, 17, 23,  3, 14, 29, 40])\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35000 0.9030479703630719 [0, 29, 12, 7, 7, 23, 40] tensor([ 0, 29, 12,  7, 13, 23, 40])\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40000 0.7609960238138834 [0, 27, 21, 5, 15, 9, 40] tensor([ 0, 27, 21,  5,  9, 40])\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45000 0.37518528529575895 [0, 35, 1, 31, 17, 24, 24, 40] tensor([ 0, 35, 25, 31, 17, 24, 40])\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "evaluation...\n",
      "[0, 22, 12, 34, 40] tensor([ 0, 22, 12, 14, 40])\n",
      "0.12\n",
      "[0, 9, 17, 28, 17, 20, 31, 35, 31, 40] tensor([ 0,  9,  6, 28, 11, 20, 31, 17, 35, 38, 40])\n",
      "epoch: 3\n",
      "0\n",
      "0 1.6667675290788924 [0, 21, 11, 22, 17, 34, 29, 29, 40] tensor([ 0, 21,  3, 22, 37, 34, 40])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-6b231eb4afb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineToIndices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategoriesToIndices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-9c339c6a6487>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(input_tensor, target_tensor)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0men_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs_num):\n",
    "    print('epoch:', epoch + 1)\n",
    "    rates = [1e-2, 1e-3, 1e-3, 1e-4, 1e-4]\n",
    "    learning_rate = rates[epoch]\n",
    "    en_optimizer = torch.optim.SGD(en.parameters(), lr=learning_rate)\n",
    "    de_optimizer = torch.optim.SGD(de.parameters(), lr=learning_rate)\n",
    "    samples_num = 50000\n",
    "    step = samples_num // 100\n",
    "    for i in range(s, samples_num):\n",
    "        if (i == 36199):\n",
    "            continue\n",
    "        input_tensor, target_tensor = lineToIndices(train[i][0]), categoriesToIndices(train[i][1]).view(-1,1)\n",
    "        loss = Train(input_tensor, target_tensor)\n",
    "        if not i % step:\n",
    "            print(i)\n",
    "        if not i % (step * 10):\n",
    "            print(i, loss, Transcript(lineToIndices(train[i][0])),categoriesToIndices(train[i][1]))\n",
    "    print('evaluation...')\n",
    "    sum = 0\n",
    "    num = 1000\n",
    "    print(Transcript(lineToIndices(train[i][0])), categoriesToIndices(train[i][1]))\n",
    "    for i in  range(samples_num, samples_num + num):\n",
    "        input_tensor, target_tensor = lineToIndices(train[i][0]), categoriesToIndices(train[i][1])\n",
    "        sum += Test(input_tensor, target_tensor)\n",
    "    print(sum/num)\n",
    "    print(Transcript(lineToIndices(train[i][0])), categoriesToIndices(train[i][1]))\n",
    "    results = np.append(results, sum/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(results.size):\n",
    "    print('epoch:', i+2, 'results:', results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 results: 0.2372\n",
      "epoch: 2 results: 0.2915\n",
      "epoch: 3 results: 0.39245\n"
     ]
    }
   ],
   "source": [
    "for i in range(results.size):\n",
    "    print('epoch:', i+1, 'results:', results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d efeat\n",
      "de feat\n",
      "def eat\n",
      "defe at\n",
      "defea t\n"
     ]
    }
   ],
   "source": [
    "def Sep(word):\n",
    "    transcription = Transcript(word)\n",
    "    print(transcription)\n",
    "    for i in range(1, len(word)):\n",
    "        word1, word2 = word[:i], word[i:]\n",
    "        print(word1, word2)\n",
    "        transcription1 = Transcript(word1)\n",
    "        print(transcription1)\n",
    "        transcription2 = Transcript(word2)\n",
    "        print(transcription2)\n",
    "        \n",
    "input = 'defeat'   \n",
    "Sep(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'defeat' [0,0,1,0,0,0]\n",
    "class SeparatorRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output).view(1,-1)\n",
    "        #output = F.log_softmax(output, dim=1)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "    \n",
    "n_hidden = 128\n",
    "se = SeparatorRNN(n_letters, n_hidden)\n",
    "se.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepTrain(input_tensor, splits):\n",
    "    en_hidden = en.initHidden()\n",
    "\n",
    "    en_optimizer.zero_grad()\n",
    "    se_optimizer.zero_grad()\n",
    "    \n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    input = torch.copy(input_tensor)\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        en_output, en_hidden = en(input_tensor[ei], en_hidden)  \n",
    "        \n",
    "    se_hidden = en_hidden\n",
    "    \n",
    "    for inp in input:\n",
    "        se_output, se_hidden = se(input, se_hidden)\n",
    "        loss += criterion(se_output, splits[i])\n",
    "        print(se_output, split[i])\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    se_optimizer.step()\n",
    "    \n",
    "    return loss.item() / input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(de, 'decoder_34')\n",
    "#torch.save(en, 'encoder_34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "max_LENGth = 0\n",
    "for t in test:\n",
    "    max_LENGth = max(max_LENGth, len(t))\n",
    "print(max_LENGth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PITCHED' 'DISSOLVERS' 'SCRAWNY' ... 'SCOGIN' 'HESSION' 'TARNOWSKI'] 41597\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv').values[:,1]\n",
    "print(test, test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for i, t in enumerate(test):\n",
    "    if not i % 1000:\n",
    "        print(i)\n",
    "    with torch.no_grad():\n",
    "        input = lineToTensor(t)\n",
    "        input = input.to(device)\n",
    "        hidden = en(input)\n",
    "        output, hidden = de(startTensor, hidden)\n",
    "        outputs = []\n",
    "        for i in range(max_length + 1):\n",
    "            output, hidden = de(output, hidden)\n",
    "            outputs = np.append(outputs, output.argmax().item())\n",
    "            if (output.argmax().item() == endTensor.item()):\n",
    "                break\n",
    "        words = np.append(words, tensorToCategories(torch.from_numpy(outputs).type(torch.LongTensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41597,)\n"
     ]
    }
   ],
   "source": [
    "print(words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "_predictions = words.reshape(words.size,)\n",
    "with open('predictions.csv', 'w') as out:\n",
    "    print('id,word', file=out)\n",
    "    for i, p in enumerate(_predictions):\n",
    "        print(i+1, p, sep=',', file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.read_csv('predictions.csv').values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41597\n"
     ]
    }
   ],
   "source": [
    "print(preds.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
